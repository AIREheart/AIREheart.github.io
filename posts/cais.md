---
title: "Systemic Thinking on AI Safety, Ethics and Society"
date: "2025-11-04"
tags: ["AI alignment", "WIP"]
excerpt: "Compilation of responses to CAIS's AISES course. Updating WIP [Nov '25 - Dec '25]"
---

**Week 1: What's a neural network, anyway?**

Neural networks are computational models that function as probabilistic pattern recognizers and predictors. They learn "what" to predict (rather than "how") based on their training stages and the path taken by their internal parameters (weights, biases) resulting in the model's optimized loss landscape. 

Depending on the model's specific architecture and what learning rules, activation and loss functions they're composed of, neural networks can perform on a wide variety of tasks ranging from supervised (where data has labelled features to map inputs to outputs), to unsupervised (where labelled features aren't present), to reinforcement learning (where iterative reward-based responses to model outputs drive a its direction and behaviors). 

As the basis for deep learning, hyper-advanced yet opaque models, these systems can grow to be highly complex complex but typically aim to prioritize accuracy in their performance which can make them unpredictable to some effect depending on novel scenarios they may encounter. 

Many questions remain to be answered, specifically on the effect of AI alignment given the the lack of interpretability, efforts towards effective scoping and scalability and manners of robustly characterizing "how" these systems learn to "think" in the first place.


**Week 2: This is an intervention.**

To better and more proactively-address organizational risks associated with AI, I believe in putting forth policies limiting and regulating model scope and scalability with respect to their *design, development and deployment*. 

With the continuous advent of hardware capacities, these regulations would shape the breadth of impact many new developments by requiring them to abide by reasoned limitations, particularly pertaining to the first two parameters: design and development of the model training stages which shape their predictive and generated outputs. Directly striking the more generalized, larger language models (LLMs), these governing efforts would be a step towards preventatively mitigating the emergence of unruly capabilities by characterizing training stages, from the datasets to validation performances, all while harnessing ethical data practices and implementing transparency where possible (or at least supporting white boxing initiatives). 

The ideal is to not dissuade progress by incentivizing more regulated development, though it's certainly possible for different groups in the landscape of multilateral developers to harbor varying alignment with these efforts based on their own metrics of success and values. Alternate consequences might be that the answers we're searching for remain elusive and uninterpretable, leading to an overcautious or irrelevant stance rather than prioritizing alternate, more-suitable interventions.



**Week 3: More critical appraisals for AI!**

One potential adversarial scenario that may arise upon deeper integration of AIs within technical fields is the liabilities to decommissioning models within the protein prediction, among other bio-modelling, spaces. Model sensitivities to small amounts of corrupt, disorderly or contradictory inputs contrasting to their trained modules results in correctively-evasive outcomes, a longstanding observation in the research of development and interpretability. 

This proclivity for change can result in unruly control or alignment, posing them as challenging tools to utilize or count on certifiably. This is particularly a challenge in the structural biology and multi-omics space where the stability landscape of various biomolecules presents a rapidly dynamic world where structures can in fact assume disordered complexes. Data enrichment (with respect to quality over quantity) becoming a budding consideration in its own right, models intrinsically possessing this mercurial flair poses them as a plausibly compromised framework scientists and engineers may struggle with placing. 

Cat or guacamole is one amusing thing when the differences still remain perceptible, but how about when differences surpass even our discernment capabilities, like when we're unsure of a crystal structure due to intrinsically disordered character (which accounts for about [40% of the eukaryotic proteome](https://pmc.ncbi.nlm.nih.gov/articles/PMC9693201/)). 

Systematically evaluating the impediments needs to be thought of at the root, shaping criteria of the model's embryology as it traverses training, fine-tuning, validation and task-specific deployment.

**Week 4: Requiem for Risk Characterization**

Black swans, considered a subset of the infrequent yet impactful tail events, refer to unforeseen rarities or incidents with drastic consequences. Also understood as a negative snowball-like turn of events, black swans are missed considerations that fly under the radar before rearing their heads at a point that upends conventional understanding for or preparedness for them. 

On the matrix of knowns by unknowns, they are particularly impervious risks since they fall under the "unknown unknown" classification and threaten our assumptions and capacities for knowledge. 


![alt text](https://github.com/AIREheart/AIREheart.github.io/blob/main/posts/image.png?raw=true)

As such, this concept reinforces our need for broadening design principles to account for buffering the unexpected, be it redundant layers of preventions and fail-safes or hyper-separation through narrowing capacities. 

As the risk evaluator's aphorism goes, we learn to differentiate a hazard from a risk on the basis of 1) its nature and 2) the degree of its exposure. When the risk becomes the status of our knowledge and understanding, the stakes grow uncertain. The field concerned with what we know and acquire our knowledge, epistemology, applies to the development and deployment of AI systems and its importance becomes highlighted as black swans reflect the fissures in our worldviews. 


Impactful mitigations against these events where we consider "hindsight being 20/20" would start with acceptance of a myriad and wide-breadth of perspectives to normalize the need for research and work, before focusing on establishing safe principles and resilient workflows. 